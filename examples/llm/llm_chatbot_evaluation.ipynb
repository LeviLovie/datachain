{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs and RAG with DataChain for chatbot evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LLM applications nowadays, the emerging standard pattern for most use-cases is to employ a pre-trained model with an API from a 3rd party provider and to augment it with a RAG context. On one hand, this means there is not much actual machine learning going on on the user's end. On the other hand naive application of \"latest and greatest\" models with no prompt engineering, testing and evaluation of RAG context can lead to needlessly expensive operational costs at best and dissapointingly poor performance at worst.\n",
    "\n",
    "Therefore, even if there is no machine learning involved, there is still a lot of fine tuning we need to do and a lot of that involves large datasets (such as histories of chatbot conversations or large collections of company documents). Just like with ML training, we need to version all that data as we finetune our applications to be able to correctly evaluate the effect of any changes we apply to our models. We can experiment with the LLM choice, prompt engineering, the way we process data for our RAG context (pre-processing, embedding, ...) and so on.\n",
    "\n",
    "In this example, we will see how we can use DataChain to create such a controlled development environment and how it can help us when we evaluate any fine-tuning of our LLM applications. We will assume we intend to build a RAG-based chatbot and them evaluate its performance.\n",
    "\n",
    "First, we will see how to use DataChain to version our RAG context datasets to preserve reproducibility of our fine-tuning experiments as the RAG context changes.\n",
    "\n",
    "Then, we will evaluate the performance of a chatbot application using a testing dataset (which we can also version with DataChain) and see how DataChain LLM integrations are going to help us in this evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing a large collection of documents for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we have a collection of relevant documents which we want to use as context in LLM queries in our chatbot application. We will be using DataChain to create, store and version vector embeddings of our documents.\n",
    "\n",
    "In this example we will be using papers from the [Neural Information Processing Systems](https://papers.neurips.cc/paper/) conference. \n",
    "\n",
    "We will proceed in the following steps:\n",
    "1. [Data ingestion with DataChain](#data-ingestion) - we will use DataChain to ingest the data, taking advantage of its lazy evaluation feature to only ingest the data we need\n",
    "1. [Data processing with the Unstructured Python library](#processing-the-documents-individually)\n",
    "1. [Scaling the data processing with DataChain](#processing-the-documents-at-scale-using-datachain-udfs)\n",
    "1. [Saving and versioning the dataset with DataChain](#saving-and-versioning-the-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from typing import List\n",
    "from collections.abc import Iterator\n",
    "\n",
    "from datachain.lib.dc import DataChain, C\n",
    "from datachain.lib.feature import Feature\n",
    "from datachain.lib.file import File\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "from unstructured.cleaners.core import clean\n",
    "from unstructured.cleaners.core import replace_unicode_quotes\n",
    "from unstructured.cleaners.core import group_broken_paragraphs\n",
    "\n",
    "from unstructured.embed.huggingface import HuggingFaceEmbeddingConfig, HuggingFaceEmbeddingEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first ingest the dataset. The data are saved on a cloud storage, so we use the `.from_storage` DataChain method. We will also use the `.filter` method to restrict ourselves only to `.pdf` files (the storage contains many other data which we do not need).\n",
    "\n",
    "Notice that:\n",
    "\n",
    "1. Since DataChain employs lazy evaluation, no data are actually loaded just yet (until we invoke an action such as showing or saving our DataChain)\n",
    "1. The previous point also means that when we filter out all non-pdf files, DataChain doesn't actually waste time loading their content only to throw them away later. This makes DataChain a lot more scalable than tools with eager evaluation.\n",
    "1. The `.from_storage` method of DataChain operates on the level of the entire bucket. This means that even if the files are stored using a complicated directory structure and potentially uploaded irregularly into this structure, we can retrieve or update our DataChain of articles with just a simple one-line command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = (\n",
    "    DataChain.from_storage(\"gs://datachain-demo/neurips\")\n",
    "    .filter(C.name.glob(\"*.pdf\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 738 rows [00:00, 6473.19 rows/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>random</th>\n",
       "      <th>vtype</th>\n",
       "      <th>dir_type</th>\n",
       "      <th>parent</th>\n",
       "      <th>name</th>\n",
       "      <th>etag</th>\n",
       "      <th>version</th>\n",
       "      <th>is_latest</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>...</th>\n",
       "      <th>file.source</th>\n",
       "      <th>file.parent</th>\n",
       "      <th>file.name</th>\n",
       "      <th>file.size</th>\n",
       "      <th>file.version</th>\n",
       "      <th>file.etag</th>\n",
       "      <th>file.is_latest</th>\n",
       "      <th>file.last_modified</th>\n",
       "      <th>file.location</th>\n",
       "      <th>file.vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>319173006993350018</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>neurips/1987/file</td>\n",
       "      <td>02e74f10e0327ad868d138f2b4fdd6f0-Paper.pdf</td>\n",
       "      <td>CPudi5uIqYcDEAE=</td>\n",
       "      <td>1721047139405563</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-07-15 12:38:59.443000+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>gs://datachain-demo</td>\n",
       "      <td>neurips/1987/file</td>\n",
       "      <td>02e74f10e0327ad868d138f2b4fdd6f0-Paper.pdf</td>\n",
       "      <td>2291566</td>\n",
       "      <td>1721047139405563</td>\n",
       "      <td>CPudi5uIqYcDEAE=</td>\n",
       "      <td>1</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>831179576623567236</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>neurips/1987/file</td>\n",
       "      <td>03afdbd66e7929b125f8597834fa83a4-Paper.pdf</td>\n",
       "      <td>CJaf6pqIqYcDEAE=</td>\n",
       "      <td>1721047138865046</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-07-15 12:38:58.917000+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>gs://datachain-demo</td>\n",
       "      <td>neurips/1987/file</td>\n",
       "      <td>03afdbd66e7929b125f8597834fa83a4-Paper.pdf</td>\n",
       "      <td>1322648</td>\n",
       "      <td>1721047138865046</td>\n",
       "      <td>CJaf6pqIqYcDEAE=</td>\n",
       "      <td>1</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>4365267491844913134</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>neurips/1987/file</td>\n",
       "      <td>072b030ba126b2f4b2374f342be9ed44-Paper.pdf</td>\n",
       "      <td>CJmztdWHqYcDEAE=</td>\n",
       "      <td>1721046993295769</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-07-15 12:36:33.340000+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>gs://datachain-demo</td>\n",
       "      <td>neurips/1987/file</td>\n",
       "      <td>072b030ba126b2f4b2374f342be9ed44-Paper.pdf</td>\n",
       "      <td>1220711</td>\n",
       "      <td>1721046993295769</td>\n",
       "      <td>CJmztdWHqYcDEAE=</td>\n",
       "      <td>1</td>\n",
       "      <td>1970-01-01 00:00:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               random vtype  dir_type             parent  \\\n",
       "0   4   319173006993350018               0  neurips/1987/file   \n",
       "1   7   831179576623567236               0  neurips/1987/file   \n",
       "2  10  4365267491844913134               0  neurips/1987/file   \n",
       "\n",
       "                                         name              etag  \\\n",
       "0  02e74f10e0327ad868d138f2b4fdd6f0-Paper.pdf  CPudi5uIqYcDEAE=   \n",
       "1  03afdbd66e7929b125f8597834fa83a4-Paper.pdf  CJaf6pqIqYcDEAE=   \n",
       "2  072b030ba126b2f4b2374f342be9ed44-Paper.pdf  CJmztdWHqYcDEAE=   \n",
       "\n",
       "            version  is_latest                    last_modified  ...  \\\n",
       "0  1721047139405563          1 2024-07-15 12:38:59.443000+00:00  ...   \n",
       "1  1721047138865046          1 2024-07-15 12:38:58.917000+00:00  ...   \n",
       "2  1721046993295769          1 2024-07-15 12:36:33.340000+00:00  ...   \n",
       "\n",
       "           file.source        file.parent  \\\n",
       "0  gs://datachain-demo  neurips/1987/file   \n",
       "1  gs://datachain-demo  neurips/1987/file   \n",
       "2  gs://datachain-demo  neurips/1987/file   \n",
       "\n",
       "                                    file.name file.size      file.version  \\\n",
       "0  02e74f10e0327ad868d138f2b4fdd6f0-Paper.pdf   2291566  1721047139405563   \n",
       "1  03afdbd66e7929b125f8597834fa83a4-Paper.pdf   1322648  1721047138865046   \n",
       "2  072b030ba126b2f4b2374f342be9ed44-Paper.pdf   1220711  1721046993295769   \n",
       "\n",
       "          file.etag file.is_latest        file.last_modified  file.location  \\\n",
       "0  CPudi5uIqYcDEAE=              1 1970-01-01 00:00:00+00:00           None   \n",
       "1  CJaf6pqIqYcDEAE=              1 1970-01-01 00:00:00+00:00           None   \n",
       "2  CJmztdWHqYcDEAE=              1 1970-01-01 00:00:00+00:00           None   \n",
       "\n",
       "  file.vtype  \n",
       "0             \n",
       "1             \n",
       "2             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[limited by 3 objects]\n"
     ]
    }
   ],
   "source": [
    "dc.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataChain created a record for each `pdf` file in the `neurips` directory, generating a `file` signal for each file. The file signal contains subsignals with metadata about each file, like `file.name` and `file.size`. Aggregate signals like `file` that contain multiple subsignals are called features.\n",
    "\n",
    "You can use the `file` feature to not only get metadata about each file, but also to open and read the file as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the documents individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to ingest the content of the pdf files as text, clean it and divide it into chunks which we will vectorize for our RAG application. We might also want to compute some additional features such as the total number of pages of an article each chunk is from.\n",
    "\n",
    "We will first do this with an example of a single pdf using the `unstructured` Python library and then we will see how we can scale this up to the entire bucket with the help of DataChain.\n",
    "\n",
    "First, we ingest and partition the pdf file and chunk it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_by_title(partition_pdf(filename=\"sample.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will apply some cleaning methods to each chunk and save the information about the total number of pages in the article corresponding to each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the chunks and add new columns\n",
    "for chunk in chunks:\n",
    "\n",
    "    chunk.apply(lambda text: clean(text, bullets=True, extra_whitespace=True, trailing_punctuation=True))\n",
    "    chunk.apply(replace_unicode_quotes)\n",
    "    chunk.apply(group_broken_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we vectorize each chung using a HuggingFace embedding encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding encoder\n",
    "\n",
    "embedding_encoder = HuggingFaceEmbeddingEncoder(\n",
    "     config=HuggingFaceEmbeddingConfig()\n",
    ")\n",
    "\n",
    "\n",
    "chunks_embedded = embedding_encoder.embed_documents(chunks)\n",
    "total_pages = chunks_embedded[-1].metadata.page_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our chunks vectorized and ready to be used in a RAG application. However, we are missing a few ingredients:\n",
    "\n",
    "1. ***Scaling*** - we only processed a single pdf file and we had to manually specify its path. We need to find a way to process all our documents at scale instead and to save the results.\n",
    "2. ***Saving and Versioning*** - even if we only had a single or a few PDF files we would like to use in our RAG, it is a good practice to version the outputs so that we can keep track of and fine-tune our RAG application. If we simply save the current results to a bucket and overwrite it each time the source is updated, we lose this. We could version the results manually, e.g. by adding a timestamp to the blob name, but that is not very reliable and will lead to unnecessary copies of files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the documents at scale, using DataChain UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use DataChain to solve the scaling and versioning issues we outline above. We will create a DataChain user-defined function (UDF) to process all our PDF files the way we did above with a single file (without us having to manually provide file paths) and save the outputs in a Datachain.\n",
    "\n",
    "The DataChain UDF functionality will allow us to generate additonal columns in our DataChain, iterating over each of the files listed in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define our pdf processing UDF, we first need to specify the output, using the DataChain `Feature` class. This will allow us to thell the UDF the output formats of the additional columns which are grouped into what is called a Feature in Datachain.\n",
    "\n",
    "```python\n",
    "class Chunk(Feature):\n",
    "    file: File\n",
    "    total_pages: int\n",
    "    text: str\n",
    "    embeddings: List[float]\n",
    "```\n",
    "\n",
    "In the above we define our custom DataChain feature called `Chunk` by specifying its content:\n",
    "\n",
    "* `file` tells DataChain to keep the keep the existing columns (which were created when we created the Datachain `ds` using the `from_storage` method)\n",
    "* `total_pages` will denote the total number of pages in the article each chunk belongs to, as before\n",
    "* `text` will contain the actual text of the chunk (after cleaning and processing)\n",
    "* `embedings` will contain the list of vecor embeddings created by the HuggingFaceedding encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now specify the input and output of our UDF using Python type signatures (these can be Feature or regular Python types)\n",
    "\n",
    "```python\n",
    "def pdf_chunks(file: File) -> Iterator[Chunk]:\n",
    "```\n",
    "Then we load each pdf file as before, the only slight change is that we use the `open` method of the DataChain `File` class to open each file.\n",
    "\n",
    "```python\n",
    "# Ingest the file\n",
    "with file.open() as f:\n",
    "    chunks = chunk_by_title(partition_pdf(file=f))\n",
    "```\n",
    "\n",
    "we then proceed with the same code as before for each chunk and them we define the content if new rows in our DataChain for each chunk (here the column names should match what we defined in our `Chunk` class)\n",
    "\n",
    "```python\n",
    "\n",
    "    # Add new rows to DataChain\n",
    "    for chunk in chunks_embedded:\n",
    "        record = {\"file\": file}\n",
    "        record[\"total_pages\"] = total_pages\n",
    "        record[\"text\"] = chunk.text\n",
    "        record[\"embeddings\"] = chunk.embeddings\n",
    "\n",
    "        yield Chunk(**record)\n",
    "```\n",
    "\n",
    "Putting all of this together, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output as a Feature class\n",
    "class Chunk(Feature):\n",
    "    file: File\n",
    "    total_pages: int\n",
    "    text: str\n",
    "    embeddings: List[float]\n",
    "\n",
    "# Use signatures to define input/output types (these can be Feature or regular Python types)\n",
    "def pdf_chunks(file: File) -> Iterator[Chunk]:\n",
    "    # Ingest the file\n",
    "    with file.open() as f:\n",
    "        chunks = chunk_by_title(partition_pdf(file=f))\n",
    "\n",
    "    # Clean the chunks and add new columns\n",
    "    for chunk in chunks:\n",
    "\n",
    "        chunk.apply(lambda text: clean(text, bullets=True, extra_whitespace=True, trailing_punctuation=True))\n",
    "        chunk.apply(replace_unicode_quotes)\n",
    "        chunk.apply(group_broken_paragraphs)\n",
    "\n",
    "    chunks_embedded = embedding_encoder.embed_documents(chunks)\n",
    "    total_pages = chunks_embedded[-1].metadata.page_number\n",
    "\n",
    "    # Add new rows to DataChain\n",
    "    for chunk in chunks_embedded:\n",
    "        record = {\"file\": file}\n",
    "        record[\"total_pages\"] = total_pages\n",
    "        record[\"text\"] = chunk.text\n",
    "        record[\"embeddings\"] = chunk.embeddings\n",
    "\n",
    "        yield Chunk(**record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains is to apply our new `pdf_chunks` UDF to the DataChain `dc`. We do that by applying the `gen` method to the DataChain with `pdf_chunks` as its parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_chunks_embeddings = dc.gen(document=pdf_chunks)\n",
    "\n",
    "dc_chunks_embeddings.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now solved our scalability issues. When using `DataChain` locally, our computation will still be restricted to a simgle machine but for larger datasets you can use the SaaS version of DataChain available through our DVC Studio which comes with automatic computation cluster management, a graphical user interface and additional ML and data versioning features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and versioning the dataset\n",
    "\n",
    "We still need to solve our versioning issues and we will also use `DataChain` to do that. To save the annotated dataset in DataChain, we will use the `.save()` method on the `dc_chunks_embeddings` dataset object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_chunks_embeddings.save(\"neurips-features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving datasets in `DataChain` allows us to:\n",
    "\n",
    "- Persist the dataset and its metadata for future use\n",
    "- Version the dataset to track changes over time\n",
    "- Share the dataset with others in our team or organization\n",
    "- Easily load the dataset in other DataChain workflows or notebooks\n",
    "\n",
    "We now have a new dataset named \"neurips-features\" in our DataChain workspace, which contains the embedded pdf data. We will later load this dataset using `DataChain.from_dataset(\"neurips-features\")` to access the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Chatbot finetuning evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will now will be using DataChain to evaluate the changes to the performance of our chatbot model. We have data We will also employ DataChain integrations with [Anthropic Claude](https://www.anthropic.com/claude) to quickly evaluate the results on a large number of queries.\n",
    "\n",
    "Our testing dataset will come from stored chatbot data with a large number user queries and bot responses. This dataset is publicly available in an Iterative GCS bucket and comes from (??? TODO: Ask Daniel where the data come from)\n",
    "\n",
    "We will use a prompt that will give use a better insight into the success of our chatbot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 79 rows [00:00, 15449.71 rows/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "source code not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n",
      "\u001b[1;32m     26\u001b[0m     explanation: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m     29\u001b[0m dc \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m     30\u001b[0m     DataChain\u001b[38;5;241m.\u001b[39mfrom_storage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://dvcx-datalakes/chatbot-public\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     31\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(C\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     40\u001b[0m     )\n",
      "\u001b[1;32m     41\u001b[0m )\n",
      "\u001b[0;32m---> 43\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pd\u001b[38;5;241m.\u001b[39moption_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\n",
      "File \u001b[0;32m~/Repos/datachain/.venv/lib64/python3.12/site-packages/datachain/query/dataset.py:1375\u001b[0m, in \u001b[0;36mDatasetQuery.to_pandas\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_pandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpd.DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;32m-> 1375\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1376\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(records)\n",
      "\u001b[1;32m   1377\u001b[0m     df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mreplace(DEFAULT_DELIMITER, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "\n",
      "File \u001b[0;32m~/Repos/datachain/.venv/lib64/python3.12/site-packages/datachain/query/dataset.py:1370\u001b[0m, in \u001b[0;36mDatasetQuery.to_records\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_records\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]:\n",
      "\u001b[0;32m-> 1370\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n",
      "\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m]\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/usr/lib64/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n",
      "\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Repos/datachain/.venv/lib64/python3.12/site-packages/datachain/query/dataset.py:1313\u001b[0m, in \u001b[0;36mDatasetQuery.as_iterable\u001b[0;34m(self, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1310\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n",
      "\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_iterable\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ResultIter]:\n",
      "\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 1313\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mselect()\n",
      "\u001b[1;32m   1314\u001b[0m         selected_columns \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m query\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "\u001b[1;32m   1315\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m ResultIter(\n",
      "\u001b[1;32m   1316\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mwarehouse\u001b[38;5;241m.\u001b[39mdataset_rows_select(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n",
      "\u001b[1;32m   1317\u001b[0m             selected_columns,\n",
      "\u001b[1;32m   1318\u001b[0m         )\n",
      "\n",
      "File \u001b[0;32m~/Repos/datachain/.venv/lib64/python3.12/site-packages/datachain/query/dataset.py:1256\u001b[0m, in \u001b[0;36mDatasetQuery.apply_steps\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1253\u001b[0m         group_by \u001b[38;5;241m=\u001b[39m step\n",
      "\u001b[1;32m   1254\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;32m-> 1256\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp_table_names\u001b[49m\n",
      "\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# a chain of steps linked by results\u001b[39;00m\n",
      "\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependencies\u001b[38;5;241m.\u001b[39mupdate(result\u001b[38;5;241m.\u001b[39mdependencies)\n",
      "\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group_by:\n",
      "\n",
      "File \u001b[0;32m~/Repos/datachain/.venv/lib64/python3.12/site-packages/datachain/query/dataset.py:708\u001b[0m, in \u001b[0;36mUDF.apply\u001b[0;34m(self, query_generator, temp_tables)\u001b[0m\n",
      "\u001b[1;32m    706\u001b[0m udf_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_udf_table(_query)\n",
      "\u001b[1;32m    707\u001b[0m temp_tables\u001b[38;5;241m.\u001b[39mappend(udf_table\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;32m--> 708\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulate_udf_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mudf_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    709\u001b[0m q, cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_result_query(udf_table, query)\n",
      "\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_result(q, cols)\n",
      "\n",
      "File \u001b[0;32m~/Repos/datachain/.venv/lib64/python3.12/site-packages/datachain/query/dataset.py:532\u001b[0m, in \u001b[0;36mUDF.populate_udf_table\u001b[0;34m(self, udf_table, query)\u001b[0m\n",
      "\u001b[1;32m    530\u001b[0m envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron)\n",
      "\u001b[1;32m    531\u001b[0m envs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONPATH\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mgetcwd()})\n",
      "\u001b[0;32m--> 532\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_feature_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocess_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mudf_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: S603\u001b[39;49;00m\n",
      "\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdatachain_exec_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--internal-run-udf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_data\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/usr/lib64/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n",
      "\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Repos/datachain/.venv/lib64/python3.12/site-packages/datachain/query/dataset.py:625\u001b[0m, in \u001b[0;36mUDF.process_feature_module\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;32m    624\u001b[0m \u001b[38;5;66;03m# Get the source code of the feature classes\u001b[39;00m\n",
      "\u001b[0;32m--> 625\u001b[0m feature_sources \u001b[38;5;241m=\u001b[39m [\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsource\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m feature_classes\u001b[38;5;241m.\u001b[39mitems()]\n",
      "\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# Set the module name for the feature classes to the generated name\u001b[39;00m\n",
      "\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m feature_classes\u001b[38;5;241m.\u001b[39mitems():\n",
      "\n",
      "File \u001b[0;32m~/Repos/datachain/.venv/lib64/python3.12/site-packages/dill/source.py:374\u001b[0m, in \u001b[0;36mgetsource\u001b[0;34m(object, alias, lstrip, enclosing, force, builtin)\u001b[0m\n",
      "\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# get source lines; if fail, try to 'force' an import\u001b[39;00m\n",
      "\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;66;03m# fails for builtins, and other assorted object types\u001b[39;00m\n",
      "\u001b[0;32m--> 374\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcelines\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menclosing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menclosing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mIOError\u001b[39;00m): \u001b[38;5;66;03m# failed to get source, resort to import hooks\u001b[39;00m\n",
      "\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force: \u001b[38;5;66;03m# don't try to get types that findsource can't get\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Repos/datachain/.venv/lib64/python3.12/site-packages/dill/source.py:345\u001b[0m, in \u001b[0;36mgetsourcelines\u001b[0;34m(object, lstrip, enclosing)\u001b[0m\n",
      "\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetsourcelines\u001b[39m(\u001b[38;5;28mobject\u001b[39m, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, enclosing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n",
      "\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    Interactively-defined objects refer to lines in the interpreter's history.\u001b[39;00m\n",
      "\u001b[1;32m    335\u001b[0m \n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    If lstrip=True, ensure there is no indentation in the first line of code.\u001b[39;00m\n",
      "\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    If enclosing=True, then also return any enclosing code.\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 345\u001b[0m     code, n \u001b[38;5;241m=\u001b[39m \u001b[43mgetblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstrip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstrip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menclosing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menclosing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m code[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], n[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\n",
      "File \u001b[0;32m~/Repos/datachain/.venv/lib64/python3.12/site-packages/dill/source.py:271\u001b[0m, in \u001b[0;36mgetblocks\u001b[0;34m(object, lstrip, enclosing, locate)\u001b[0m\n",
      "\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetblocks\u001b[39m(\u001b[38;5;28mobject\u001b[39m, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, enclosing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, locate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;32m    262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n",
      "\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    Interactively-defined objects refer to lines in the interpreter's history.\u001b[39;00m\n",
      "\u001b[1;32m    264\u001b[0m \n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    DEPRECATED: use 'getsourcelines' instead\u001b[39;00m\n",
      "\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 271\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ismodule(\u001b[38;5;28mobject\u001b[39m):\n",
      "\u001b[1;32m    274\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lstrip: lines \u001b[38;5;241m=\u001b[39m _outdent(lines)\n",
      "\n",
      "File \u001b[0;32m~/Repos/datachain/.venv/lib64/python3.12/site-packages/dill/source.py:122\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n",
      "\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IS_IPYTHON \u001b[38;5;129;01mand\u001b[39;00m is_module_main:\n",
      "\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m#FIXME: quick fix for functions and classes in IPython interpreter\u001b[39;00m\n",
      "\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 122\u001b[0m         file \u001b[38;5;241m=\u001b[39m \u001b[43mgetfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    123\u001b[0m         sourcefile \u001b[38;5;241m=\u001b[39m getsourcefile(\u001b[38;5;28mobject\u001b[39m)\n",
      "\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m/usr/lib64/python3.12/inspect.py:917\u001b[0m, in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n",
      "\u001b[1;32m    915\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m\n",
      "\u001b[1;32m    916\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;32m--> 917\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource code not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is a built-in class\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mobject\u001b[39m))\n",
      "\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismethod(\u001b[38;5;28mobject\u001b[39m):\n",
      "\n",
      "\u001b[0;31mOSError\u001b[0m: source code not available"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datachain.lib.claude import claude_processor\n",
    "from datachain.lib.dc import C, DataChain\n",
    "from datachain.lib.feature import Feature\n",
    "\n",
    "MODEL = \"claude-3-opus-20240229\"\n",
    "PROMPT = \"\"\"Consider the dialogue between the 'user' and the 'bot'. \\\n",
    "The 'user' is a human trying to find the best mobile plan. \\\n",
    "The 'bot' is a chatbot designed to query the user and offer the \\\n",
    "best  solution. The dialog is successful if the 'bot' is able to \\\n",
    "gather the information and offer a plan, or inform the user that \\\n",
    "such plan does not exist. The dialog is not successful if the \\\n",
    "conversation ends early or the 'user' requests additional functions \\\n",
    "the 'bot' cannot perform. Read the dialogue below and rate it 'Success' \\\n",
    "if it is successful, and 'Failure' if not. After that, provide \\\n",
    "one-sentence explanation of the reasons for this rating. Use only \\\n",
    "JSON object as output with the keys 'status', and 'explanation'.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Rating(Feature):\n",
    "    status: str = \"\"\n",
    "    explanation: str = \"\"\n",
    "\n",
    "\n",
    "dc = (\n",
    "    DataChain.from_storage(\"gs://dvcx-datalakes/chatbot-public\", type=\"text\")\n",
    "    .filter(C.name.glob(\"*.txt\"))\n",
    "    .settings(parallel=3)\n",
    "    .limit(5)\n",
    "    .map(claude=claude_processor(prompt=PROMPT, model=MODEL))\n",
    "    .map(\n",
    "        rating=lambda claude: Rating(\n",
    "            **(json.loads(claude.content[0].text) if claude.content else {})\n",
    "        ),\n",
    "        output=Rating,\n",
    "    )\n",
    ")\n",
    "\n",
    "dc.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
